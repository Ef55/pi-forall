\newif\ifcomments     %% include author discussion
\commentsfalse        %% toggle comments here

\documentclass{article}

\usepackage{bigfoot}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[dvipsnames]{xcolor}
\usepackage{verbatim}
\usepackage[utf8]{inputenc}
\usepackage[hyphens]{url}
\usepackage[hidelinks]{hyperref}
\usepackage{supertabular}
\usepackage{listings}
\usepackage{xspace}
\usepackage{ottalt}

\newcommand\scw[1]{\ifcomments\emph{\textcolor{violet}{#1}}\fi}

% requires dvipsnames
\usepackage{lstpi}
\usepackage{lsthaskell}
\newcommand\cd[1]{\lstinline[language=Haskell]{#1}}



 
\newcommand\pif{\texttt{pi-forall}\xspace}
\newcommand\unbound{\texttt{Unbound}\xspace}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]


\title{Implementing Dependent Types in \pif}
\author{Stephanie Weirich}

\begin{document}

\maketitle

% load Ott generated latex macros, and modify them with ottalt.sty
\inputott{pi-rules}

\section{Goals}

These lecture notes describe the implementation of a small dependently-typed
language called ``\pif'' and walk through the implementation of its type
checker. They are based on lectures given at the \emph{Oregon Programming
  Languages Summer School} during June 2022, and derived from earlier lectures
from summer school lectures during 2014 and 2013.  \footnote{NOTE: if the date
  on this document is before June 27, 2022, then these notes are still in
  draft form. If you find any errors or typos, please send me a pull request
  at \url{https://github.com/sweirich/pi-forall}.}

\paragraph{What do I expect from you, dear reader?} These notes assume a
familiarity with the basics of the lambda calculus, including its standard
operations (alpha-equivalence, substitution, evaluation) and the basics of
type systems (specification of type systems using inference rules). For
background on this material, I recommend~\cite{tapl, pfpl}.

Furthermore, these lectures and notes also refer to an implementation of a
demo type checker and assume basic knowledge of the Haskell programming
language. This implementation is available at
\url{https://github.com/sweirich/pi-forall} and is written using the Haskell
programming language. As you study these notes, I encourage you to download
this code and experiment with it. Detailed installation instructions are
available with the source code.

\begin{figure}[h]
\begin{center}
\begin{tabular}{llll}
Key feature & \pif version      & Section\\
\hline
Core system & \texttt{version1} & Sections~\ref{sec:simple}, \ref{sec:bidirectional}, and \ref{sec:implementation} \\
Equality    & \texttt{version2} & Sections~\ref{sec:equality} and ~\ref{sec:pattern-matching}\\
Irrelevance & \texttt{version3} & Section ~\ref{sec:irrelevance} \\
Datatypes   & \texttt{full}     & Section ~\ref{sec:datatypes} \\
\end{tabular}
\end{center}
\caption{Connection between sections and \pif versions}
\label{fig:impls}
\end{figure}

\paragraph{What do these notes cover?}
These notes are broken into several sections that incrementally build up the
design and implementation of a type checker for a dependently-typed
programming language.


If you are looking at the repository, you'll see that it includes several
incrementally more expressive implementations in separate
subdirectories. These implementations build on each other (each is an
extension of the previous) and are summarized in Figure~\ref{fig:impls}.  As
you read each chapter, refer to its corresponding implementation to see how the
features described in that chapter can be implemented.


\begin{itemize}
\item Section~\ref{sec:simple} presents the mathematical specification of the
  core type system including its concrete syntax (as found in \pif source
  files), abstract syntax, and core typing rules (written using standard
  mathematical notation). This initial specification of the type system is
  simple and declarative. It \emph{specifies} what terms should type check,
  but cannot be directly implemented.
  
\item Section~\ref{sec:bidirectional}, then reformulates the typing rules so
  that they are \emph{syntax-directed} and specify a type checking
  algorithm. The key idea of this section is to recast the typing rules as a
  \emph{bidirectional type system} that specifies where type annotations are
  required.

%  As you read this section, ask yourself ``How much information do
%  we need to include in terms to make type checking algorithmic?''

\item Section~\ref{sec:implementation} introduces the core \pif implementation
  and walks through the type checker found in \texttt{version1}, the Haskell
  implementation of the typing rules discussed in
  Section~\ref{sec:bidirectional}. This section shows how the \unbound library
  can assist with variable binding and automatically derive operations for
  capture-avoiding substitution and alpha-equivalence. This section also
  describes a monadic structure for the type checker, and how it can help with
  the production of error messages, the primary purpose of a type checker.

% Ask yourself ``How can we represent the
%  abstract syntax of the language, including variable binding?''

\item Section~\ref{sec:equality} discusses the role of definitional equality
  in dependently typed languages. After motivating examples, it presents both
  a specification of when terms are equal and a semi-decidable algorithm that
  can be incorporated into the type checker. 
%  Ask yourself ``How can we know when types (and terms) are equal?''

\item An important feature of dependently-typed languages is the ability for
  run time tests to be reflected into the type
  system. Section~\ref{sec:pattern-matching} shows how to extend \pif with a
  simple form of dependent pattern matching.

\item Section~\ref{sec:irrelevance} introduces the idea of tracking the
  \emph{relevance} of arguments. Relevance tracking enables parts of the
  program to be identified as ``compile-time only'' and thus erased before
  execution. It also identifies parts of terms that can be ignored when
  deciding equivalence. 

% Ask yourself ''How can we decide what parts
%  of the term are irrelevant during computation? Can be ignored when checking
%  for equality?''

\item Finally, Section~\ref{sec:datatypes} introduces arbitrary datatypes and
  generalizes dependent pattern matching. This section combines the features
  introduced in the previous sections (Sections \ref{sec:equality},
  \ref{sec:pattern-matching}, and \ref{sec:irrelevance}) into a single unified
  language feature.
\end{itemize}


\paragraph{What do these notes \textbf{not} cover?}
The goal of this notes is to provide an introductory overview. As a result,
there are many topics related to the implementation of dependent type theories
and programming languages that are not included here. Furthermore, because
these notes come with a reference implementation, they emphasize only a single
point in the design space.

\begin{itemize}
\item For simplicity, the \pif language does not enforce termination through
  type checking. Implementing a proof system like Agda or Coq requires
  additional structure, including universe levels and bounded iteration.
\item Many implementations of dependent type theories use
  \emph{normalization-by-evaluation} to decide whether types are equivalent
  during type checking.  \pif uses an alternative approach based on
  weak-head-normalization, defined using substitution. This approach is closer
  to $\lambda$-calculus theory, but can be less efficient in practice.
\item Also for simplicity, \pif does not attempt to infer arguments, using
  unification or other means. As a result, example programs in \pif are
  significantly more verbose than in other languages.
\item This implementation relies on the \unbound library for variable binding,
  alpha-equivalence and substitution.  As a result, most issues related to
  this topic are avoided.
\item Recent work on cubical type theory, higher-inductive types and
  univalence is not covered here.
\end{itemize}

Section~\ref{sec:related-work} includes a discussion of related tutorials,
including how this one differs, as well as pointers for where to
go for more information.

\paragraph{What do I want you to get out of all of this?}

\begin{enumerate}
\item An understanding of how to translate mathematical specifications of type
  systems and logics into code, i.e. how to represent the syntax of a
  programming language and how to implement a type checker. More generally,
  this involves techniques for turning a declarative specification of a system
  of judgments into an algorithm that determines whether the judgement holds.
   
\item Exposure to the design choices of dependently-typed languages. In this
  respect, my goal is breadth not depth. As a result, I will provide
  \emph{simple} solutions to some of the problems that we face and sidestep
  other problems entirely. Because solutions are chosen for simplicity, the
  end of these notes includes pointers if you want to go deeper.
   
\item Experience with the Haskell programming language. I think Haskell is an
  awesome tool for this sort of work and I want to demonstrate how its
  features monads, generic programming) are well-suited for this task.

\item A tool that you can use as a basis for experimentation. When you design
  your language, how do you know what programs you can and cannot express?
  Having an implementation lets you work out (smallish) examples and will help
  convince you (and your reviewers) that you are developing something useful.
  Please use \pif as a starting point for your ideas.

\item Templates and tools for writing about type systems. The source files for
  these lecture notes are available in the same repository as the demo
  implementation and I encourage you to check them out.\footnote{See the
    \texttt{doc} subdirectory in \url{https://github.com/sweirich/pi-forall}.}
  Building these notes requires Ott~\cite{ott}, a tool specifically tailored
  for typesetting type systems and mathematical specifications of programming
  languages.
\end{enumerate}


\section{A Simple Core Language}
\label{sec:simple}

Let's consider a simple dependently-typed lambda calculus. What should it
contain? At the bare minimum we start with the following five forms:

\[
\begin{array}{rcll}
     [[a]],[[b]],[[A]],[[B]] & ::=& [[x]]  &\mbox{ variables  }\\
         &&[[\x. a]]          &\mbox{ lambda expressions (anonymous functions)} \\
         &&[[a b]]            &\mbox{ function applications }\\
         &&[[(x:A) -> B]]     &\mbox{ dependent function type, aka $\Pi$ }\\
         &&[[Type]]           &\mbox{ the `type' of types}\\
\end{array}
\]

As in many dependently-typed languages, we have the \emph{same} syntax for
both expressions and types. For clarity, I'll used lowercase letters $a$ for
expressions and uppercase letters for their types $A$.

Note that $\lambda$ and $\Pi$ above are \emph{binding forms}. They bind the
variable $x$ in $a$ and $B$ respectively.

\subsection{When do expressions in this language type check?}

We define the type system for this language using an inductive
relation shown in Figure~\ref{fig:typing}. This relation is between an
expression $[[a]]$, its type $[[A]]$, and a typing context $[[G]]$. 

\[ \fbox{$[[ G |- a : A ]]$} \]

A typing context $[[G]]$ is an ordered list of assumptions about variables and
their types.  

    \[ [[ G]]  ::= \emptyset\ |\ [[G, x : A]] \]

We will assume that each of the variables in this list are
distinct from each other, so that there will always be at most one assumption
about any variable's type.\footnote{On paper, this assumption is reasonable
  because we always extend the context with variables that come from binders,
  and we can always rename bound variables so that they differ from other
  variables in scope. Any implementation of the type system will need to somehow 
  make sure that this invariant is maintained.}

\paragraph{An initial set of typing rules: Variables and Functions}

\begin{figure}[t]
\drules[t]{$[[G|-a:A]]$}{Core type system}{var,lambda,app,pi,type}
\caption{Typing rules for core system}
\label{fig:typing}
\end{figure}

Consider the first two rules shown in Figure~\ref{fig:typing}, \rref{t-var}
and \rref{t-lambda}.  The first rule states that the types of variables are
recorded in the typing context. The premise $[[x:A elem G]]$ requires us to
find an association between $x$ and the type $A$ in the list $[[G]]$.

The second rule, for type checking functions, introduces a new variable into
the context when we type check the body of a $\lambda$-expression. It also
requires $[[A]]$, the type of the function's argument to be a valid type.

\paragraph{Example: Polymorphic identity functions}

Note that in \rref{t-lambda}, the parameter $x$ is allowed to appear in the
result type of the function, $[[B]]$. Why is this useful? Well, it gives us
\emph{parametric polymorphism} automatically.  In Haskell, we write the
identity function as follows, annotating it with a polymorphic type.

\begin{haskell}
id :: forall a. a -> a
id = \y -> y
\end{haskell}

\noindent
Because the type of \cd{id} is generic we can apply this function to any type of
argument.  

We can also write a polymorphic identity function in \pif, as follows.
\begin{piforall}
id : (x:Type) -> (y : x) -> x
id = \x. \y. y
\end{piforall}

This definition is similar to Haskell, but with a few modifications. First,
\pif{} uses a single colon for type annotations and a \cd{.} for function
binding instead of Haskell's quirky \cd{::} and \cd{->}. Second, the \pif
definition of \texttt{id} uses two lambdas: one for the ``type'' argument $x$
and one for the ''term'' argument $y$. In \pif, there is no syntactic
difference between these arguments: both are arguments to the identity
function. 

Finally, the \pif{} type of \cd{id} uses the dependent function type, with
general form $[[(x:A) -> B]]$. This form is like Haskell's usual function type
$[[A -> B]]$, except that we can name the argument $[[x]]$ so that it can be
referred to in the body of the type $[[B]]$. In \cd{id}, dependency allows the
the type argument $[[x]]$ to be used later as the type of $[[y]]$.  We call
types of this form $\Pi$-types. (Often dependent function types are written as
$\Pi x\!:\!A. B$ in formalizations of dependent type theory.\footnote{The
  terminology for these types is muddled: sometimes they are call dependent
  function types and sometimes they are called dependent product types. We use
  the non $\Pi$-notation to emphasize the connection to functions.})

The fact that the type of $x$ is $[[Type]]$ means that $x$ plays the role of a
type variable, such as \texttt{a} in the Haskell type. Because we don't have a
syntactic distinction between types and terms, in \pif we say that ``types''
are anything of type $[[Type]]$ and ``terms'' are things of type $A$ where $A$
has type $[[Type]]$.

We can use the typing rules to construct a typing derivation for the identity
function as follows.

\[
\inferrule*
{
   \inferrule*
   {
              \inferrule*{ [[y:x elem (x:Type,y:x)]]}{[[x:Type, y:x |- y : x]]} 
      \qquad \inferrule*{[[ x:Type elem (x:Type) ]] }{[[ x:Type |- x : Type ]]}
   }{
     [[x:Type |- \y. y : (y : x) -> x]]
   }  \qquad \inferrule*{}{[[ |- Type : Type ]]}
}{
  [[ |- \x. \y. y : (x:Type) -> (y : x) -> x ]] 
}
\]

This derivation uses \rref{t-lambda} to type check the two $\lambda$-expressions, before using the variable 
rule to ensure that both the body of the function $[[x]]$ and its type $[[y]]$ are 
well-typed. Finally, this rule also uses \rref{t-type} to show that $[[Type]]$ itself 
is a valid type, see below.


\paragraph{More typing rules:  Types}

Observe in the typing derivation above that the rule for typing
lambda-expressions has second precondition: we need to make sure that
when we add assumptions $x:A$ to the context, that $A$ really is a type.
Without this precondition, the rules would allow us to derive this nonsensical
type for the polymorphic identity function.

\[   [[   |- \x.\y. y : (x: True) -> (y:x) -> x ]] \]

This precondition means that we need some rules that conclude that types are
actually types. For example, the type of a function is itself a type, so we
will declare it so with \rref{t-pi}. This rule also ensures that the domain
and range of the function are also types.

Likewise, for polymorphism we need the somewhat perplexing \rref{t-type}, that
declares, by fiat, that $[[Type]]$ is a valid $[[Type]]$.\footnote{Note that,
  this rule make our language inconsistent as a logic, as it can encode
  Girard's paradox. More about this in section~\ref{sec:related-work}.}

\paragraph{More typing rules: Applications}

The application rule, \rref{t-app}, requires that the type of the argument
matches the domain type of the function. However, note that because the body
of the function type $B$ could have $x$ free in it, we need also need to
substitute the argument $[[b]]$ for $x$ in the result.

\paragraph{Example: applying the polymorphic identity function}

In \pif we should be able to apply the polymorphic identity function to
itself. When we do this, we need to first provide the type of \cd{id},
producing an expression of type
$[[(y: ((x:Type) -> (y:x) -> x)) -> ((x:Type) -> (y:x) -> x)]]$.  This
function can take \cd{id} as an argument.

\begin{piforall}
    idid : (x:Type) -> (y : x) -> x 
    idid = id ((x:Type) -> (y : x) -> x) id
\end{piforall}

\paragraph{Example: Church booleans}

Because we have (impredicative) polymorphism, we can \emph{encode} familiar
types, such as booleans. The idea behind this encoding, called a Church
encoding, is to represent terms by their eliminators. In other words, what is
important about the value true?  The fact that when you get two choices, you
pick the first one.  Likewise, false ``means'' that with the same two choices,
you should pick the second one.  With parametric polymorphism, we can give the
two terms the same type, which we'll call bool.

\begin{piforall}
    bool : Type
    bool = (x : Type) -> x -> x -> x

    true : bool
    true = \x. \y. \z. y
	 
    false : bool
    false = \x. \y. \z. z
\end{piforall}

\noindent
Thus, a conditional expression just takes a boolean and returns it.

\begin{piforall}
    cond : bool -> (x:Type) -> x -> x -> x
    cond = \b. b 
\end{piforall}

\paragraph{Example: Logical ``and'' }

We can also encode a logical ``and'' data structure using a Church encoding.

\begin{piforall}
    and : Type -> Type -> Type
    and = \p. \q. (c: Type) -> (p -> q -> c) -> c

    conj : (p:Type) -> (q:Type) -> p -> q -> and p q
    conj = \p.\q. \x.\y. \c. \f. f x y

    proj1 : (p:Type) -> (q:Type) -> and p q -> p
    proj1  = \p. \q. \a. a p (\x.\y.x)

    proj2 : (p:Type) -> (q:Type) -> and p q -> q
    proj2  = \p. \q. \a. a q (\x.\y.y)

    and_commutes : (p:Type) -> (q:Type) -> and p q -> and q p
    and_commutes = \p. \q. \a. conj q p (proj2 p q a) (proj1 p q a)
\end{piforall}

\section{From typing rules to a typing algorithm}
\label{sec:bidirectional}

The rules that we have developed so far are great for saying \emph{what} terms
should type check, but they don't say \emph{how} type checking works. We've
developed these rules without thinking about how we would implement them.

A set of typing rules, or \emph{type system}, is called \emph{syntax-directed}
if it is readily apparent how to interpret that collection of typing rules as
code. In other words, for some type systems, we can directly translate them to 
some Haskell function.

For this type system, we would like to implement the following Haskell
function, that when given a term and a typing context, represented as a list
of pairs of variables and their types, produces the type of the term, if it
exists.\footnote{Note: If you are looking at the \pif implementation, note
  that this is not the final type of \cd{inferType}.}

\begin{haskell}
type Ctx = [(Var,Type)]

inferType :: Term -> Ctx -> Maybe Type
\end{haskell}

Let's look at our rules in Figure~\ref{fig:typing}.  Is the definition of this
function straightforward?  For example, in the variable rule, as long as we
can look up the type of a variable in the context, we can produce its
type. That means that, assuming that there is some function \cd{lookupTy},
with type \cd{Ctx -> Var -> Maybe Type}, this rule corresponds to the
following case of the \cd{inferType} function. So far so good!

\begin{haskell}
inferType (Var x) ctx = lookupTy ctx x
\end{haskell}
	
\noindent		
Likewise, the case of the typing function for the $[[Type]]$ term is also
straightforward. When we see the term $[[Type]]$, we know immediately that it
is its own type.

\begin{haskell}
inferType Type ctx = Just Type
\end{haskell}

The only stumbling block for the algorithm is \rref{t-lambda}. To type check a
function, we need to type check its body when the context has been extended
with the type of the argument. But, like Haskell, the type of the argument
$[[A]]$ is not annotated on the function in \pif. So where does it come from?

There is actually an easy fix to turn our current system into an algorithmic
one. We could just annotate $\lambda$-expressions with the types of the
abstracted variables.  But, to be ready for future extension, we'll do
something else.

Look at our example \pif code above: the only types that we wrote were the
types of definitions. It is good style to do that. Furthermore, there is
enough information there for type checking---wherever we define a function, we
can look at those types to know what type its argument should have.  So, by
changing our point of view, we can get away without annotating lambdas with
those argument types.

\subsection{A bidirectional type system}

Let's redefine the system using two judgments. The first one is similar to the
judgement that we saw above, and we will call it type
\emph{inference}.\footnote{The term \emph{type inference} usually refers to
  much more sophisticated deduction of an expressions type, in the context of
  much less information encoded in the syntax of the language. We're not doing
  anything difficult here, just noting that we can read the judgment with
  $[[A]]$ as an output. } This judgement will be paired with (and will depend
on) a second judgement, called type \emph{checking}, that takes advantage of
known type information, such as the annotations on top-level definitions.

We express these judgements using the notation defined in
Figure~\ref{fig:bidirectional} and implement them in Haskell using the
mutually-recursive functions \texttt{inferType} and
\texttt{checkType}. Furthermore, to keep track of which rule is in which
judgement, rules that have inference as a conclusion start with
\textsc{i-} and rules that have checking as a conclusion start with
\textsc{c-}.

\begin{figure}
\drules[i]{$[[G |- a => A]]$}{in context $[[G]]$, infer that term $a$ has type $A$}
{var,app-simple,pi,type,annot}
\drules[c]{$[[G |- a <= A]]$}{in context $[[G]]$, check that term $a$ has type $A$}
{lambda,infer-simple} 
\caption{(Simple) Bidirectional type system}
\label{fig:bidirectional}
\end{figure}

Let's compare these rules with our original typing rules. For \rref{i-var},
we need to only change the colon to an inference arrow. The context tells us
the type to infer. 

On the other hand, in \rref{c-lambda} we should check $\lambda$-expressions
against a known type. If that type is provided, we can propagate it to the
body of the lambda expression. We also want to check that $A$ is a $[[Type]]$.

The rule for applications, \rref{i-app-simple}, is in inference mode. Here, we
first infer the type of the function, but once we have that type, we may use
it to check the type of the argument. This mode change means that
$\lambda-$expressions that are arguments to other functions (like
\texttt{map}) do not need any annotation.

For types, it is apparent their type is $[[Type]]$, so \rref{i-pi,i-type} just
continue to infer that.
	  
Notice that this system is incomplete. There are inference rules for every
form of expression except for lambda. On the other hand, only lambda
expressions can be checked against types.  We can make the checking judgement
more applicable with \rref{c-infer-simple} that allows us to use inference
whenever a checking rule doesn't apply.

Now, let's think about the reverse problem a bit. There are programs that the
checking system won't admit but would have been acceptable by our first
system. What do they look like?

Well, they involve applications of explicit lambda terms:

\[
\inferrule*[Right=t-app]
{
      [[ |- \x.x : Bool -> Bool]]  \qquad  [[|- True : Bool ]]
}{
      [[ |- (\x.x) True : Bool ]]
}
\]

This term doesn't type check in the bidirectional system because application
requires the function to have an inferable type, but lambdas don't.
%
However, there is not that much need to write such terms. We can always
replace them with something equivalent by doing a $\beta$-reduction of the
application (in this case, just replace the term with $[[True]]$).

In fact, the bidirectional type system has the property that it only checks
terms in \emph{normal} form, i.e. those that do not contain any
$\beta$-reductions. 

\paragraph{Type annotations}
To type check nonnormal forms in \pif, we also add typing annotations as a new
form of expression to \pif, written $[[(a : A)]]$, and add \rref{i-annot} to
the type system.

Type annotations allow us to supply known type information anywhere, not just
at top level. For example, we can construct this derivation.

\[
\inferrule*[Right=i-app]
{
      [[ |- (\x.x : Bool -> Bool) => Bool -> Bool]]  \qquad  [[|- True <= Bool ]]
}{
      [[ |- (\x.x : Bool -> Bool) True => Bool ]]
}
\]

The nice thing about the bidirectional system is that it reduces the number of
annotations that are necessary in programs that we want to write. As we will
see, checking mode will be even more important as we add more terms to the
language.

Furthermore, we want to convince ourselves that the bidirectional system
checks the same terms as the original type system. This means that we want to
prove a property like this one:

\begin{lemma}[Correctness of the bidirectional type system]
If $[[G |- a => A]]$ then $[[G |- a : A]]$.
If $[[G |- a <= A]]$ then $[[G |- a : A]]$.
\end{lemma}

On the other hand, the reverse property is not true. Even if there exists some
typing derivation $[[G |- a : A]]$ for some term $[[a]]$, we may not be able
to infer or check that term using the algorithm. However, all is not lost:
there will always be some term $[[a']]$ that differs from $[[a]]$ only in the
addition of typing annotations that can be inferred or checked instead.

One issue with this bidirectional system is that it is not closed under
substitution.  What this means is that given some term $[[b]]$ with a free
variable, $[[ G, x :A |- b <= B]]$, and another term $[[a]]$ with the same
type $[[ G |- a <= A]]$ of that variable, we \emph{do not} have a derivation
$[[ G |- b[a/x] <= A]]$. The reason is that types of variables are always
inferred, but the term $[[a]]$, may need the type $[[A]]$ to be supplied to
the type checker.  This issue is particularly annoying in \rref{i-app} when we
replace a variable (inference mode) with a term that was validated in checking
mode.

As a result, our type system infers types, but the types that are inferred do
not have enough annotations to be checked themselves. We can express the property
that does hold about our system, using this lemma:

\begin{lemma}[Regularity]
If $[[G |- a => A]]$ then $[[G |- A : Type]]$.
\end{lemma}

This issue is not significant, but we could resolve it by adding an annotation
before substitution. However, in our implementation of \pif, we do not do so
to reduce clutter.

\section{Putting it all together in a Haskell
implementation}
\label{sec:implementation}

In the previous section, we defined a bidirectional type system for a small
core language. Here, we'll start talking about what the implementation of this
language looks like in Haskell.

First, an overview of the main files of the implementation. There are a few
more source files in the repository (in the subdirectory \cd{src/}), but these
are the primary ones.

\begin{verbatim}
 Syntax.hs      - specification of the AST
 Parser.hs      - turn strings into AST
 PrettyPrint.hs - displays AST in a (somewhat) readable form
 Main.hs        - top-level routines (repl)
  
 Environment.hs - defines the type checking monad         
 TypeCheck.hs   - implementation of the bidirectional type checker
\end{verbatim}

\subsection{Variable binding using the \unbound library
  {[}Syntax.hs{]}}


One difficulty with implementing any sort of lambda calculus is the treatment
of variable binding. Functions ($\lambda$-expressions) and their types
($\Pi$-types) \emph{bind} variables. In the implementation of our type
checker, we'll need to be able to determine whether two terms are
\emph{alpha-equivalent}, calculate the \emph{free variables} of a term, and
perform \emph{capture-avoiding substitution.} When we work with a
$\lambda$-expression, we will want to be sure that the binding variable is
\emph{fresh}, that is, distinct from all other variables in the program or in
the typing context.

In the \pif implementation, we use the \unbound library to get all of these
operations for free. This works because we will use types from this library 
in the definition of the abstract syntax of \pif, and those types will specify 
the binding structure of \pif expressions. 

First, the \unbound library defines a type for variable names, called
\texttt{Name} and we use this type to define \cd{TName}, the type of term
names in our AST.
%
\begin{verbatim}
-- | Type used for variable names in Terms
type TName = Unbound.Name Term
\end{verbatim}
%
This type is indexed by \cd{Term} the AST type that it is a name for. That
way \unbound can make sure that we only substitute the right form of syntax
for the right name, i.e. we can only replace a \texttt{TName} with a
\texttt{Term}.
The \unbound library includes an overloaded function \texttt{subst\ x\ a\ b},
which means $[[ b[a/x] ]]$\footnote{See Guy Steele's talk about the notation
  for substitution~\cite{steele:ppop17}. This is the most common mathematical
  notation for this operation.}, i.e. substitute $[[a]]$ for $[[x]]$ in
$[[b]]$. 

In general, we will want to apply a substitution to many different sorts of
syntax. For example, we may want to substitute a term $[[a]]$ for a term name
$[[x]]$ in all of the terms that appear in some typing context. Therefore,
\unbound's substitution function has a type with the following general
pattern for \cd{subst a x b}.
\begin{verbatim}
 class Subst a b where
    subst  :: Name a -> a -> b -> b       
\end{verbatim}

The \texttt{subst} function in this class ensures that when we see that
\texttt{a} is the right sort of thing to stick in for \texttt{x}. The library
can automatically generate instances of the \texttt{Subst} class.

With term names, we can define the abstract syntax that corresponds to our
language above, using the following datatype.

\begin{verbatim}
data Term

  = -- | type of types  `Type`
    Type

  | -- | variables  `x`
    Var TName

  | -- | abstractions  `\x. a`
    Lam (Unbound.Bind TName Term)

  | -- | applications `a b`
    App Term Arg

  | -- | function types   `(x : A) -> B`
    Pi (Unbound.Bind (TName, Unbound.Embed Type) Type)

  | -- | Annotated terms `( a : A )`
    Ann Term Type

  ...
  deriving (Show, Generic)

-- | An argument to a function
data Arg = Arg {unArg :: Term}
  deriving (Show, Generic, Unbound.Alpha, Unbound.Subst Term)
\end{verbatim}

As you can see, variables are represented by names. \unbound's \texttt{Bind}
type constructor declares the scope of the bound variables. Both
\texttt{Lam} and \texttt{Pi} bind a single variable in a \texttt{Term}.

Because the syntax is all shared, a \texttt{Type} is just another
name for a \texttt{Term}. We'll use this name just for documentation.

\begin{verbatim}
  type Type = Term
\end{verbatim}

We also isolate function arguments as a special type \texttt{Arg}, which is
isomorphic to \texttt{Term}. This distinction is not terribly useful for the
initial version of \pif, but is important for future extensions.

The definitions of the \texttt{Arg} datatype automatically derives instances
for two type classes from the \texttt{unbound-generics} library:
\texttt{Unbound.Alpha} and \texttt{Unbound.Subst}.

Among other things, the \texttt{Alpha} class enables functions for alpha
equivalence and free variable calculation, with the types shown below.
Because \unbound creates these instances for us, we don't have to worry
about defining them.

\begin{verbatim}
aeq :: Alpha a => a -> a -> Bool
\end{verbatim}

For \texttt{Term}, we do not use the default definitions of the \texttt{Alpha}
and \texttt{Subst} classes. Instead, we need to provide a bit more
information.  First, we would like the definition of alpha-equivalence of
terms to ignore type annotations (as well as other practicalities, such as
source code positions for error messages and explicit parentheses). Therefore 
our instance does so for these constructors of the \texttt{Term} data type and 
defers to the generic definition for the rest. 

The \texttt{Subst} instance for \texttt{Term} requires telling \unbound where
the variables:

\begin{verbatim}
instance Unbound.Subst Term Term where
  isvar (Var x) = Just (SubstName x)
  isvar _ = Nothing  
\end{verbatim}

For more information about \unbound, see the
\href{https://hackage.haskell.org/package/unbound-generics}{unbound-generics
  hackage page}.

\subsection{A Type Checking monad
{[}Environment.hs{]}}

Recall that our plan is to write two mutually recursive functions for
type checking of the following types:

\begin{verbatim}
inferType :: Term -> Ctx -> Maybe Type
 
checkType :: Term -> Type -> Ctx -> Bool
\end{verbatim}

The inference function should take a term and a context and if it type
checks, produce its type and its elaboration (where all annotations have
been filled in). The checking function should take a term and a context
and a type, and if that term has that type produce an elaborated version
(where all of the annotations have been filled in.)

Well actually, we'll do something a bit different. We'll define a
\emph{type checking monad}, called \texttt{TcMonad} that will handle the
plumbing for the typing context, and allow us to return more information
than \texttt{Nothing} when a program doesn't type check. Therefore 
our two functions will have this type: hiding the context and part

\begin{verbatim}
inferType :: Term -> TcMonad (Term,Type)
 
checkType :: Term -> Type -> TcMonad Term
\end{verbatim}

Those of you who have worked with Haskell before may be familiar with the
monads
\href{https://hackage.haskell.org/package/mtl-2.1.2/docs/Control-Monad-Reader.html}{MonadReader},
and the
\href{https://hackage.haskell.org/package/mtl-2.1.2/docs/Control-Monad-Error.html}{MonadError},
which our type checking monad will be instances of.

\begin{verbatim}
lookupTy :: TName -> TcMonad Term
extendCtx :: Decl -> TcMonad Term -> TcMonad Term
 
  
err  :: (Disp a) => a -> TcMonad b
warn :: (Disp a) => a -> TcMonad b
\end{verbatim}

We'll also need this monad to be a freshness monad, to support working
with binding structure, and throw in MonadIO for good measure.

\subsection{Implementing the Type Checking Algorithm
{[}Typecheck.hs{]}}

Now that we have the type checking monad available, we can start our
implementation. For flexibility \texttt{inferType} and
\texttt{checkType} will \emph{both} be implemented by the same function:

\begin{verbatim}
inferType :: Term -> TcMonad Type
inferType t = tcTerm t Nothing
 
checkType :: Term -> Type -> TcMonad ()
checkType tm ty = void $ tcTerm tm (Just ty) 
\end{verbatim}

The \texttt{tcTerm} function checks a term, producing its type. The
second argument is \texttt{Nothing} in inference mode and an expected
type in checking mode.

\begin{verbatim}
tcTerm :: Term -> Maybe Type -> TcMonad Type
\end{verbatim}

The general structure of this function starts with a pattern match for
the various syntactic forms in inference mode:

\begin{verbatim}
tcTerm (Var x) Nothing = ... 
 
tcTerm Type Nothing = ...

tcTerm (Pi bnd) Nothing = ...
 
tcTerm (App t1 t2) Nothing = ...
 
\end{verbatim}

Mixed in here, we also have a pattern for lambda expressions in checking
mode:

\begin{verbatim}
tcTerm (Lam bnd) (Just (Pi bnd2)) = ... 
 
tcTerm (Lam _) (Just nf) =  -- checking mode wrong type
   err [DS "Lambda expression has a function type, not ", DD nf]
\end{verbatim}

There are also several cases for practical reasons (annotations, source code
positions, etc.) and a few cases for homework.

Finally, the last case covers all other forms of checking mode, by
calling inference mode and making sure that the inferred type is equal
to the checked type. This case is the implementation of \rref{c-infer}.

\begin{verbatim}
tcTerm tm (Just ty) = do
 ty' <- inferType tm 
 unless (Unbound.aeq ty' ty) $
   err [DS "Types don't match", DD ty, DS "and", DD ty']
 return ty    
\end{verbatim}

The function \texttt{aeq} ensures that the two types are alpha-equivalent. If
they are not, it stops the computation and throws an error.

\subsection{Example}

The \pif source file \href{version1/pi/Lec1.pi}{\tt Lec1.pi} contains the examples
that we worked out in Section~\ref{sec:simple}. Let's try to type check it,
after filling in the missing code in \texttt{TypeCheck.hs}.

\subsection{Exercise (Type Theory \& Haskell) - Add Booleans and
Sigma types}

Some fairly standard typing rules for booleans are shown below.

\drules{$[[G|-a:A]]$}{Booleans}{t-bool,t-true,t-false,t-if-weak}

Likewise, we can also extend the language with $\Sigma$-types.

\drules{$[[G|-a:A]]$}{Sigma-types}{t-sigma,t-pair,t-letpair-weak}

A Sigma-type is a product where the type of the second component of the
product can depend on the first component.  We destruct Sigmas using pattern
matching. A simple rule for pattern matching introduces variables into the
context when pattern matching the sigma type. These variables are not allowed
to appear free in the result type of the pattern match.

This part of the homework has two parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  First: rewrite the rules above in bidirectional style. Which rules
  should be inference rules? Which ones should be checking rules? If you
  are familiar with other systems, how do these rules compare?
\item
  In Haskell, later: The code in \texttt{version1/} includes abstract
  and concrete syntax for booleans and sigma types. The \pif file
  \texttt{version1/test/Hw1.pi} contains examples of using these new
  forms. However, to get this file to compile, you'll need to fill in
  the missing cases in \texttt{version1/src/TypeCheck.hs}.
\end{enumerate}


\section{Equality in Dependently-Typed Languages}
\label{sec:equality}

You may have noticed in the previous sections that there was something
missing. Most of the examples that we did could have also been written in
System F (or something similar)!

Next, we are going to think about how adding a notion of definitional equality
can make our language more expressive.

The addition of definitional equality follows several steps. First, we will see why we even want this feature in the language in the first place. Next, we will create a declarative specification of definitional equality and extend our typing relation with 
a new rule that enables it to be used. After that, we'll talk about algorithmic versions of both the equality relation and how to introduce it into the algorithmic type system. 
Finally, we'll cover modifications to the Haskell implementation.

\subsection{Motivating Example: Type level reduction}

In full dependently-typed languages (and in full \pif) we can see the need for
definitional equality. We want to equate types that are not merely
alpha-equivalent, so that more expressions type check.

We saw yesterday an example where we wanted a definition of equality
that was more expressive than alpha-equivalence. Recall our encoding for
the logical \texttt{and} proposition:

\begin{piforall}
and : Type -> Type -> Type
and = \p. \q. (c: Type) -> (p -> q -> c) -> c
\end{piforall}

Unfortunately, our definition of \texttt{conj} still doesn't type check:

\begin{piforall}
conj : (p:Type) -> (q:Type) -> p -> q -> and p q
conj = \p.\q. \x.\y. \c. \f. f x y
\end{piforall}

Running this example with \texttt{version1} of the type checker produces
the following error:

\begin{verbatim}
Checking module "Lec1"
Type Error:
../test/Lec1.pi:34:22:
    Function a should have a function type. Instead has type and p q
    When checking the term 
       \p . \q . \a . a p ((\x . \y . x))
    against the signature
       (p : Type) -> (q : Type) -> (and p q) -> p
    In the expression
       a p ((\x . \y . x))
\end{verbatim}

The problem is that even though we want \texttt{and\ p\ q} to be equal
to the type
\texttt{(c:\ Type)\ -\textgreater{}\ (p\ -\textgreater{}\ q\ -\textgreater{}\ c)\ -\textgreater{}\ c}
the type checker does not treat these types as equal.

Note that the type checker already records in the environment that
\texttt{and} is defined as
\texttt{\textbackslash{}p.\textbackslash{}q.\ (c:\ Type)\ -\textgreater{}\ (p\ -\textgreater{}\ q\ -\textgreater{}\ c)\ -\textgreater{}\ c}.
We'd like the type checker to look up this definition when it sees the
variable \texttt{and} and beta-reduce this application.

\subsection{Another example needing more expressive
equality}

As another example, in the full language, we might have a type of length
indexed vectors, where vectors containing values of type \texttt{A} with
length \texttt{n} can be given the type \texttt{Vec\ A\ n}. In this
language we may have a safe head operation, that allows us to access the
first element of the vector, as long as it is nonzero.

\begin{piforall}
head : (A : Nat) -> (n : Nat) -> Vec A (succ n) -> Vec A n
head = ...
 
\end{piforall}

However, to call this function, we need to be able to show that the
length of the argument vector is equal to \texttt{succ\ n} for some
n.~This is ok if we know the length of the vector outright

\begin{piforall}
v1 : Vec Bool (succ 0)
v1 = VCons True VNil
 
\end{piforall}

So the application \texttt{head\ Bool\ 0\ v1} will type check. (Note
that \pif cannot infer the types \texttt{A} and \texttt{n}.)

However, if we construct the vector, its length may not be a literal
natural number:

\begin{piforall}
append : (n : Nat) -> (m : Nat) -> Vec A m -> Vec A n -> Vec A (plus m n)
append = ...
\end{piforall}

In that case, to get \texttt{head\ Bool\ 1\ (append\ v1\ v1)} to type
check, we need to show that the type \texttt{Vec\ Bool\ (succ\ 1)} is
equal to the type \texttt{Vec\ Bool\ (plus\ 1\ 1)}. If our definition of
type equality is \emph{alpha-equivalence}, then this equality will not
hold. We need to enrich our definition of equality so that it equates
more terms.

\subsection{Defining definitional equality}

The main idea is that we will:

\begin{itemize}
\item
  establish a new judgement that defines when types are equal

\[ \fbox{$[[ G |- A = B]]$} \]
\item
  add the following rule to our type system so that it works ``up-to''
  our defined notion of type equivalence

\[ \drule[width=3in]{t-conv} \]

\item
  Figure out how to revise the \emph{algorithmic} version of our type
  system so that it supports the above rule.
\end{itemize}

What is a good definition of equality? We started with a very simple
one: alpha-equivalence. But we can do better. Check out the rules in Figure~\ref{fig:defeq}.

\begin{figure}
\drules[e]{$[[G |- A = B]]$}{Definitional Equality}{beta,refl,sym,trans,pi,lam,app,subst}
\caption{Definitional equality for core \pif}
\label{fig:defeq}
\end{figure}

\Rref{e-beta} ensures that our relation \emph{contains
  beta-equivalence}. \Rref{e-refl,e-sym,e-trans} makes sure that this relation
is an \emph{equivalence relation}. Furthermore, this relation should be a
\emph{congruence relation} (i.e.~if subterms are equal, then larger terms are
equal), as verified by rules \rref{e-pi,e-lam,e-app}.  Finally, we want to be
sure that this relation has ``functionality'' (i.e.~we can lift
equalities). We declare so using \rref{e-subst}.


(Note: if we add booleans and sigma types, we will also need corresponding 
$\beta$-equivalence rules and congruence rules for those constructs.)

\subsection{Using definitional equality in the algorithm}

We would like to consider our type system as having the following rule:

\[ \drule[width=4in]{t-conv} \] 

But that rule is not syntax-directed. This rule could apply in any place in a
derivation. Where do we need to add equality preconditions in our
bidirectional system? It turns out that there are only a few places.

\begin{itemize}
\item
  Where we switch from checking mode to inference mode in the algorithm.
  Here we need to ensure that the type that we infer is the same as the
  type that is passed to the checker.

\[ \drule[width=4in]{c-infer} \]

In this case, our equality algorithm must, when given two terms, decide
whether they are equal. In \pif we will use a semi-decision procedure based 
on reducing terms to normal forms. However, because reduction may not terminate, 
our equality checking function could diverge.

\item
  In the rule for application, when we infer the type of the function we
  need to make sure that the function actually has a function type. But
  we don't really know what the domain and co-domain of the function
  should be. We'd like our algorithm for type equality to be able to
  figure this out for us. 
 
\[ \drule[width=4in]{i-app} \]

In this case, we are given a single term and we need to know whether it is
equivalent to some $\Pi$-type. Because $\Pi$-types are \emph{head} forms, we
can do this via reduction. Just evaluate the type $A$ to its head form, using
the rules shown in Figure~\ref{fig:whnf}. If that form is a $\Pi$-type, then
we can access its domain type.

\begin{figure}
\drules[whnf]{$[[whnf a = v]]$}{}{lam-beta,let-beta,type,lam,pi,var,lam-cong}
\caption{Weak-head normal form reduction}
\label{fig:whnf}
\end{figure}

For closed terms, these rules correspond to a big-step evaluation relation.
This relation is semi-decidable, so we can express it as a Haskell function
that may diverge. 


\end{itemize}

\subsection{Algorithmic definitional equality}

The rules above \emph{specify} when terms should be equal, but they are
not an algorithm. We actually need several different functions. First,

\begin{verbatim}
equate :: Term -> Term -> TcMonad ()
\end{verbatim}

ensures that the two provided types are equal, or throws a type error if
they are not. This function corresponds directly to our definition of
type equality.

Second, we also need to be able to determine whether a given type is equal to
some ``head'' form, without knowing exactly what that form is.  For example,
when \emph{checking} lambda expressions, we need to know that the provided
type is of the form of a $\Pi$-type ($[[(x:A)-> B]]$). Likewise, when inferring
the type of an application, we need to know that the type inferred for the
function is actually a $\Pi$-type.

We can determine this in two ways. Most directly, the function

\begin{verbatim}
ensurePi :: Type -> TcMonad (TName, Type, Type)
\end{verbatim}

checks the given type to see if it is equal to some pi type of the form
$[[(x:A1)-> A2]]$, and if so returns \texttt{x},
\texttt{A1} and \texttt{A2}.\\
This function is defined in terms of a helper function, that implements the 
rules shown in Figure~\ref{fig:whnf}.

\begin{verbatim}
whnf :: Term -> TcMonad Term
 
\end{verbatim}

This function reduces a type to its \emph{weak-head normal form (whnf)}. Such
terms have done all of the reductions to the outermost lambda abstraction (or
$\Pi$) but do not reduce subterms. In other words:

\begin{piforall}
 (\x.x) (\x.x)  
  
\end{piforall}

is not in whnf, because there is more reduction to go to get to the
head. On the other hand, even though there are still internal reductions
possible:

\begin{piforall}
  \y. (\x.x) (\x.x)   
\end{piforall}

and

\begin{piforall}
 (y:Type) -> (\x.x) Bool 
\end{piforall}

are in weak head normal form. Likewise, the term \texttt{x\ y} is also in weak
head normal form, because, even though we don't know what the head form is, we
cannot reduce the term any more.

In \texttt{version2} of the the
\href{version2/src/TypeCheck.hs}{implementation}, these functions are
called in a few places: - \texttt{equate} is called at the end of
\texttt{tcTerm} - \texttt{ensurePi} is called in the \texttt{App} case
of \texttt{tcTerm} - \texttt{whnf} is called in \texttt{checkType},
before the call to \texttt{tcTerm} to make sure that we are using the
head form in checking mode.

\subsection{Implementing definitional equality (see Equal.hs)}

There are several ways for implementing definitional equality, as stated
via the rules above. The easiest one to explain is based on
reduction---for \texttt{equate} to reduce the two arguments to some
normal form and then compare those normal forms for equivalence.

One way to do this is with the following algorithm:

\begin{verbatim}
 equate t1 t2 = do 
    nf1 <- reduce t1  -- full reduction, throughout term
    nf2 <- reduce t2  
    Unbound.aeq nf1 nf2
\end{verbatim}

However, we can do better. We'd like to only reduce as much as
necessary. Sometimes we can equate the terms without completely reducing
them.

\begin{verbatim}
  equate t1 t2 = do
     if (Unbound.aeq t1 t1) then return () else do
      nf1 <- whnf t1  -- reduce only to 'weak head normal form'
      nf2 <- whnf t2
      case (nf1,nf2) of 
        (App a1 a2, App b1 b2) -> 
            -- make sure subterms are equal
            equate a1 b1 >> equate a2 b2
        (Lam bnd1, Lam bnd2) -> do
            -- ignore variable names, but use the name 
            -- fresh name for both lambda bodies
            (_, b1, _, b2) <- unbind2Plus bnd1 bnd2
             equate b1 b2
        (_,_) -> err ...
\end{verbatim}

Therefore, we reuse our mechanism for reducing terms to weak-head normal
form.

Why weak-head reduction vs.~full reduction?

\begin{itemize}
\item
  We can implement deferred substitutions for variables. Note that when
  comparing terms we need to have the definitions available. That way we
  can compute that \texttt{(plus\ 3\ 1)} weak-head normalizes to 4, by
  looking up the definition of \texttt{plus} when needed. However, we
  don't want to substitute all variables through eagerly---not only does
  this make extra work, but error messages can be extremely long.
\item Furthermore, we allow recursive definitions in \pif, so normalization
  may just fail completely. However, this definition based on \texttt{whnf}
  only unfolds recursive definitions when necessary, and then only once, so
  avoids some infinite loops in the type checker.

  Note that we don't have a complete treatment of equality. There will always
  be terms that can cause \texttt{equate} to loop forever.  On the other hand,
  there will always be terms that are not equated because of conservatively in
  unfolding recursive definitions.
\end{itemize}


\section{Dependent pattern matching and propositional equality}
\label{sec:pattern-matching}

\subsection{Refining  rules for if and $\Sigma$-type elimination}

Consider our elimination rules for if:


\[ \drule[width=4in]{t-if-weak} \]

We can do better by making the result type \texttt{A} depend on whether the
condition is true or false.

\[ \drule[width=4in]{t-if-full} \]

For example, here is a simple definition that requires this rule:

\begin{piforall}
-- function from booleans to types
T : Bool -> Type
T = \b. if b then Unit else Bool

-- returns Unit when the argument is True
bar : (b : Bool) -> T b
bar = \b . if b then () else True     
\end{piforall}

It turns out that this rule is difficult to implement. It is not
syntax-directed because $A$ and $x$ are not fixed by the syntax. Given
$[[A[True/x] ]]$ and $[[A [False/x] ]]$ and $[[ A [a/x] ]]$ (or anything that
they are definitionally equal to!)  how can we figure out whether they
correspond to each other?

So, we'll not be so ambitious in \pif. We'll only allow this refinement when
the scrutinee is a variable, deferring to the weaker, non-refining typing rule
for if in all other cases.

\[ \drule[width=4in]{t-if} \]

And, in the our bidirectional system, we'll only allow refinement
when we are in checking mode.

\[ \drule[width=4in]{c-if} \]

To implement this rule, we need only remember that $x$ is $[[True]]$ or
$[[False]]$ when checking the individual branches of the if expression.
\footnote{
Here is an alternative version, for inference mode only, suggested
during lecture:

\[ \drule[width=4in]{i-if-alt} \]

It has a nice symmetry---if expressions are typed by if types. Note however,
to make this rule work, we'll need a stronger definitional equivalence
than we have. In particular, we'll want our definition of equivalence to
support the following equality:

\[ \drule[width=4in]{e-if-eta} \]

That way, if the type of the two branches of the if does not actually
depend on the boolean value, we can convert the \texttt{if} expression
into a more useful type.
}

We can modify the elimination rule for $\Sigma$-types similarly.

\[ \drule[width=4in]{c-letpair} \]

This modification changes our definition of $\Sigma$-types from weak $\Sigma$s
to strong $\Sigma$s. With either typing rule, we can define the first
projection.

\begin{piforall}
fst : (A:Type) -> (B : A -> Type) -> (p : { x2 : A | B x2 }) -> A
fst = \A B p. let (x,y) = p in x
\end{piforall}

But, weak Sigmas cannot define the second projection. The
following code only type checks using the above rule.

\begin{piforall}
snd : (A:Type) -> (B : A -> Type) -> (p : { x2 : A | B x2 }) -> B (fst A B p)
snd = \A B p. let (x,y) = p in y
\end{piforall}

(Try this out using version2 of the implementation and the Lec2.pi 
input file.)

\subsection{Propositional equality}

\begin{figure}
\drules[t]{$[[G |- a : A]]$}{Typing}{refl,eq,subst,contra}
\caption{Propositional equality}
\label{fig:propeq}
\end{figure}

You started proving things right away in Coq or Agda with an equality
proposition. For example, in Coq, when you say
\begin{verbatim}
Theorem plus_O_n : forall n : nat, 0 + n  = n.
\end{verbatim}
\noindent
you are using a built in type, \texttt{a\ =\ b} that represents the
proposition that two terms are equal.

As a step towards more general indexed datatypes, we'll start by adding
just a propositional equality type to \pif.

The main idea of the equality type is that it converts a \emph{judgement} that
two types are equal into a \emph{type} that evaluates to a value only when two
types are equal.\footnote{Recall that all types are inhabited by an infinite
  loop in \pif.}  In other words, the new value form $[[refl]]$ has 
type $[[a = b]]$, as shown in \rref{t-refl}.  

Sometimes, you might see the rule written as follows:
%
\[ \drule[width=3in]{t-refl-alt} \]
%
However, this rule will turn out to be equivalent to the above version.

An equality type is well-formed when the terms on both sides of the equality
have the same type. In other words, when it implements \emph{homogeneous}
equality, as shown in rule \rref{t-eq}.

The elimination rule for propositional equality is written $[[subst a by b]]$
in \pif.  This term allows us to convert the type of one expression to
another, as shown in rule \rref{t-subst-simple} below. In this rule, we can change the type
of some expression $[[a]]$ by replacing an occurrence of some term $[[a1]]$
with an equivalent type. 

\[ \drule[width=4in]{t-subst-simple} \]

How can we implement this rule? For simplicity, we'll play the same trick that
we did with booleans, requiring that one of the sides of the equality be a
variable.

\[ \drule[width=4in]{c-subst-left} \]
\[ \drule[width=4in]{c-subst-right} \]

Note that this elimination form for equality is powerful. We can use it to
show that propositional equality is symmetric and transitive.

\begin{piforall}
sym : (A:Type) -> (x:A) -> (y:A) -> (x = y) -> y = x
trans : (A:Type) -> (x:A) -> (y:A) -> (z:A)
      -> (x = z) -> (z = y) -> (x = y)
\end{piforall}

Furthermore, we can also extend the elimination form for propositional
equality with dependent pattern matching as we did for booleans. This
corresponds to the elimination rule \rref{t-subst}, that observes that the
only way to construct a proof of equality is with the term $[[refl]]$. (This
version of subst is very close to an eliminator for propositional equality
called \texttt{J}).

As above, this rule (and the corresponding left rule) only
applies when $b$ is also a variable.

\[ \drule[width=4in]{c-subst-right-refl} \]

One last addition: \texttt{contra}. If we can somehow prove a false statement,
then we should be able to prove anything. A contradiction is a
proposition between two terms that have different head forms. For now,
we'll use \rref{t-contra}, shown in Figure~\ref{fig:propeq}.

\subsubsection{Homework (\pif: more church encodings)}

The file \texttt{version2/test/NatChurch.pi} is a start at a Church
encoding of natural numbers. Replace the TRUSTMEs in this file so that
it compiles.

\subsubsection{Homework (\pif: equality)}

Complete the file \href{version2/test/Hw2.pi}{\texttt{Hw2.pi}}. This file gives
you practice with working with equality propositions in \pif.

\section{Irrelevance: the $\forall$ of \pif}
\label{sec:irrelevance}

Now, let's talk about erasure. In dependently typed languages, some arguments
are ``ghost'' or ``specificational'' and only there for proofs. For efficient
executables, we don't want to have to ``run'' these arguments, nor do we want
them taking up space in data structures.

Functional languages do this all the time: they erase \emph{type
annotations} and \emph{type} arguments before running the code. This
erasure makes sense because of parametric polymorphic functions are not
allowed to depend on types. The behavior of map must be the same no
matter whether it is operating on a list of integers or a list of
booleans.

In a dependently-typed language we'd like to erase types too. And proofs
that are only there to make things type check. Coq does this by making a
distinction between \texttt{Prop} and \texttt{Set}. Everything in
\texttt{Set} stays around until run time, and is guaranteed not to depend
on \texttt{Prop}. \scw{TODO: talk about squash, etc.}

We'll take another approach.

In \pif we have new kind of quantification, called ``forall'', that
marks erasable arguments. We mark forall quantified arguments with
brackets. For example, we can mark the type argument of the polymorphic
identity function as erasable.

\begin{piforall}
id : [x:Type] -> (y : x) -> x
id = \ [x] y. y
\end{piforall}

When we apply such functions, we'll put the argument in brackets too, so
we remember that \texttt{id} is not really using that type.

\begin{piforall}
t = id [Bool] True
\end{piforall}

However, we need to make sure that irrelevant arguments really are
irrelevant. We wouldn't want to allow this definition:

\begin{piforall}
id' : [x:Type] -> [y:x] -> x
id' = \ [x][y]. y
\end{piforall}

Here \texttt{id'} claims that its second argument is
erasable, but it is not.

\subsection{How do we rule this out in the type system?}

We need to make sure that an irrelevant variable, like $x$ above, is not
``used'' in the body of a $\lambda$-abstraction. How can we do so?

The approach we will take will seem a bit round about at first. But this 
approach is the most future-proof, for reasons that we discuss below. 

The key idea is that we will mark variables in the context with an
$[[epsilon]]$ annotation, which is either $[[Rel]]$ or $[[Irr]]$.  ``Normal''
variables, introduced by $\lambda$-expressions will always be marked as
relevant ($[[Rel]]$). Only relevant variables can be used in terms and 
we revise the variable typing rule to require this annotation:

\[ 
\drule{t-evar}
\]

An irrelevant abstraction introduces its variable into the context tagged with
$[[Irr]]$. Because of this tag, this variable will be inaccessible in the body
of the function.

\[
\drule{t-elambda}
\]

However, this variable should be available for use in \emph{types} and other
\emph{irrelevant} parts of the term. To enable this use, we use a special
context operation $[[G Irr]]$, as in the second premise in \rref{t-elambda}, and
in the \rref{t-eapp} rule below.

\[
\drule{t-eapp}
\]

This operation on the context, called \emph{demotion}, converts all $[[Irr]]$
tags on variables to be $[[Rel]]$. It represents a shift in our perspective:
the variables that were not visible before are now available after this
context modification. 

Finally, when checking irrelevant $\Pi$ types, we mark the variable with $[[Rel]]$ when 
we add it to the context so that it may be used in the range of the $\Pi$ type. Otherwise, we would not be able to verify the type of the polymorphic identity function above.
\[
\drule{t-epi}
\]

\subsection{Compile-time irrelevance}

What does checking irrelevance buy us? Because we know that irrelevant
arguments are not actually used by the function, we know that they can be
\emph{erased} prior to execution. We don't need this information to compute
the answer, so why provide it?

An additional benefit is during equivalence checking. When deciding whether 
two terms are equal, we don't need to look at their irrelevant components. 

\[ 
\drule{e-eapp}
\]

We've been alluding to this the whole time, but now we'll come down to
it. We're actually \emph{defining} equality over just the computationally
relevant parts of the term instead of the entire term. In \texttt{version3} of the 
implementation, note how the definition
of \texttt{equate} ignores arguments that are tagged as `irrelevant'. 
(And from the beginning, our system already ignores types that appear only in 
type annotations. Our justification for doing this is the same.)

Why is compile-time irrelevance important? 
\begin{itemize}
\item faster comparison: don't have to look at the
whole term when comparing for equality. Coq / Adga look at type
annotations 
\item more expressive: don't have to \emph{prove} that those
parts are equal
\end{itemize}

For example, consider the example below. The function $p$ has an irrelevant
argument, so it must be a constant function. Another name for $p$ might be a
\emph{phantom} type.  Therefore it is sound to equate any two applications of
$p$ because we know that we will always get the same result.

\begin{piforall}
irrelevance : (p : [i : Nat] -> Type) -> p [1] = p [2]
irrelevance = \p . Refl
\end{piforall}


However, note that casting is a relevant use of propositional equality. In the 
example below, \pif will prevent us from marking the argument \texttt{pf} as 
irrelevant. 

\begin{piforall}
proprel : [a : Type] -> (pf : a = Bool) -> (x : a) -> Bool
proprel = \ [a] pf x . 
  subst x by pf 
\end{piforall}

The reason for this restriction is because the language includes
non-termination. We don't know whether this proof is \texttt{Refl} or some
infinite loop. So this argument must be evaluated to be sure that the two
types are actually equal. If (somehow) we knew that the argument would always
evaluate to \texttt{Refl} we could erase it.

\section{Datatypes and Indexed Datatypes}
\label{sec:datatypes}

Finally, we'll add datatypes with erasable arguments to \pif.
The code to look at is the ``complete'' implementation in
\href{full/}{\texttt{full}}.

Unfortunately, datatypes are both:

\begin{itemize}
\item
  Really important (you see them \emph{everywhere} when working with
  languages like Coq, Agda, Idris, etc.)
\item
  Really complicated (there are a \emph{lot} of details). In general, datatypes 
  subsume booleans, $\Sigma$-types, propositional equality, and can carry 
  irrelevant arguments. So they combine all of the complexity of the previous
  sections in one construct.
\end{itemize}

Unlike the previous sections, where we could walk through all of the details
of the specification of the type system, not to mention its implementation, we
won't be able to do that here. There is just too much! The goal of this part
is to give you enough information so that you can pick up the Haskell code and
understand what is going on.

Even then, realize that the implementation that I'm giving you is not the
complete story! Recall that we're not considering termination. That means that
we can think about eliminating datatypes merely by writing recursive
functions; without having to reason about whether those functions
terminate. Coq, Agda and Idris include a lot of machinery for this termination
analysis, and we won't cover any of it.

We'll work up the general specification of datatypes piece-by-piece,
generalizing from features that we already know to more difficult cases.
We'll start with ``simple'' datatypes, and then extend them with both
parameters and indices.

\subsection{``Dirt simple'' datatypes}\label{dirt-simple-datatypes}

Our first goal is simple. What do we need to get the simplest examples of
non-recursive and recursive datatypes working? By this I mean datatypes that
you might see in Haskell or ML, such as \texttt{Bool}, \texttt{Void} and
\texttt{Nat}.

\subsubsection{Booleans}\label{booleans}

For example, one homework assignment was to implement booleans. Once we
have booleans then we can

\begin{piforall}
  data Bool : Type where
     True 
     False
\end{piforall}

In the homework assignment, we used \texttt{if} as the elimination form
for boolean values.

\begin{piforall}
 not : Bool -> Bool
 not = \ b . if b then False else True
\end{piforall}

For uniformity, we'll have a common elimination form for all datatypes,
called \texttt{case} that has branches for all cases. (We'll steal
Haskell syntax for case expressions, including layout.) For example, we
might rewrite \texttt{not} with case like this:

\begin{piforall}
not : Bool -> Bool
not = \ b . 
  case b of 
       True -> False
       False -> True
\end{piforall}

\subsubsection{Void}\label{void}

The simplest datatype of all is one that has no constructors!

\begin{piforall}
data Void : Type where {}
\end{piforall}

Because there are no constructors, the elimination form for values of
this type doesn't need any cases!

\begin{piforall}
false_elim : (A:Type) -> Void -> A
false_elim = \ A v . case v of {} 
\end{piforall}

Void brings up the issue of \emph{exhaustiveness} in case analysis. Can
we tell whether there are enough patterns so that all of the cases are
covered? This is something that our implementation should be able to do.

\subsubsection{Nat}\label{nat}

Natural numbers include a data constructor with an argument. For
simplicity in the parser, those parentheses must be there.

\begin{piforall}
data Nat : Type where
   Zero
   Succ of (Nat)
\end{piforall}

In case analysis, we can give a name to that argument in the pattern.

\begin{piforall}
is_zero : Nat -> Bool
is_zero = \ x . case x of 
   Zero -> True
   Succ n -> False
   
\end{piforall}

\subsubsection{Dependently-typed data constructor args}

Now, even in our ``dirt simple'' system, we'll be able to encode
some new structures, beyond what is available in functional programming
languages like Haskell and ML. These structures won't be all that useful
yet, but as we add parameters and indices to our datatypes, they will
be. For example, here's an example of a datatype declaration where the
data constructors have dependent types.

\begin{piforall}
data SillyBool : Type where      
   ImTrue  of (b : Bool) (_ : b = True)
   ImFalse of (b: Bool)  (_ : b = False)
\end{piforall}

\subsection{Implementing the type checker for ``dirt simple'' datatypes}

Datatype declarations, such as \texttt{data\ Bool}, \texttt{data\ Void}
or \texttt{data\ Nat} extend the context with new type constants (aka
type constructors) and new data constructors. It is as if we had added a
bunch of new typing rules to the type system, such as:

\drules[t]{$[[G|- a : A]]$}{Typing}{Nat,Void,Zero,Succ,ImTrue}

In the general form, a \emph{simple} data type declaration includes a
name and a list of data constructors.

\begin{piforall}
   data T : Type where
      K1                 -- no arguments
      K2 of (A)          -- single arg of type A
      K3 of (x:A)        -- also single arg of type A, called x for fun
      K4 of (x:A)(y:B)   -- two args, the type of B can mention A
      K5 of (x:A)[x = a] -- one arg, plus constraint about that arg
\end{piforall}

In fact, each data constructor takes a special sort of list of arguments that
we call a \emph{telescope}. (The word telescope for this structure was coined
by de Bruijn~\cite{debruijn} to describe the scoping behavior of this
structure. The scope of each variable overlaps all of the subsequent ones,
nesting like an expandable telescope.)

We can represent this structure in our implementation by adding a new
form of declaration (some parts have been elided compared to
\texttt{full}, so we're building up to that version.)

\begin{verbatim}
-- | type constructor names
type TCName = String

-- | data constructor names
type DCName = String

data Decl = 
    -- | Declaration for the type of a term `x : A`
    TypeSig Sig   
  | -- | The definition of a particular name `x = a`
    Def TName Term
    ...
   
  | -- | Datatype definition  `data T = ...`
    Data TCName [ConstructorDef]

-- | A Data constructor has a name and a telescope of arguments
data ConstructorDef = ConstructorDef DCName Telescope

-- | A telescope is a list of type declarations and definitions
newtype Telescope = Telescope [Decl]

\end{verbatim}

For example, a declaration for the \texttt{Bool} type would be the following.
\begin{verbatim}
boolDecl :: Decl 
boolDecl = Data "Bool" [ConstructorDef "False" [], 
                        ConstructorDef "True"  [] ]                                    
\end{verbatim}


\subsection{Checking (simple) data constructor
applications}

\begin{figure}


% \[ 
%\begin{array}{llcl}
%\mathit{Telescope} &[[ D ]]& ::= &[[ x : A , D ]]\ |\ [[ x = b , D ]]\ |\ \mathit{empty} \\
%\mathit{Pattern}   &[[ p ]]& ::= &[[ x ]]\ |\ [[K ps]] \\
%\end{array}
% \]

\begin{tabular}{lll}
$[[G |- args <= D]]$   & \texttt{tcArgTele} 
   & Type check a list of arguments against a telescope \\
$[[G |- p : A => D]]$ & \texttt{declarePat} 
   & Create telescope containing all of the variables from the pattern \\
$[[ |- a ~ b => D]]$  & \texttt{unify} 
   & Compare two terms to create a list of definitions \\
$[[ D [ a / x ] ]] $  & \texttt{doSubst}   
   & Substitute through a telescope \\
$[[ D1 [ args / D2 ] ]]$ & \texttt{substTele}
   & Substitute a list of args for the variables declared in a telescope \\
\end{tabular}
\caption{Functions for checking datatypes and case expressions}
\label{fig:data}
\end{figure}

When we have a datatype declaration, that means that new data type $[[T]]$ of
type $[[Type]]$ will be added to the context.  Furthermore, the context should
record all of the type constructors for that type, $[[Ki]]$, as well as the
telescope, written $[[Di]]$, for that data constructor. This information will
be used to check terms that are the applications of data constructors. For
simplicity, we'll assume that data constructors must be fully applied to all
of their arguments.

So our typing rule looks a little like this. We have $[[args]]$ as
representing the list of arguments for the data constructor $[[K]]$.

\[ \drule{i-dcon-simple} \]

We need to check that list against the telescope for the constructor, using
the judgment $[[G |- args <= D]]$.  

In this judgement, each argument must have
the right type. Furthermore, because of dependency, we substitute that
argument for the variable in the rest of the telescope.

\[ \drule[width=3in]{tele-sig} \]

Furthermore, we also substitute when the telescope contains definitions. 

\[ \drule[width=3in]{tele-def} \]

When we get to the end of the argument list (i.e.~there are no more arguments) we
should also get to the end of the telescope.

\[ \drule{tele-nil} \]

In \texttt{TypeCheck.hs}, the function \texttt{tcArgTele} essentially
implements this judgement. (For reasons that we explain below, we have a
special type \texttt{Arg} for the arguments to the data constructor.)

\begin{verbatim}
tcArgTele :: [Arg] -> Telescope -> TcMonad [Arg]
\end{verbatim}

This function relies on the substitution function for
telescopes, written $[[D [ a / x] ]]$ in the rules above:

\begin{verbatim}
doSubst :: [(TName, Term)] -> [Assn] -> TcMonad [Assn]
\end{verbatim}


\subsection{Eliminating dirt simple datatypes}

In your homework assignment, we used a special \texttt{if} to eliminate boolean types.
Now, we'd like to replace that with the more general \texttt{case} expression, of the 
form $[[case a of { </ pi -> ai // i /> }]]$ 
that works with any form of datatype. What should the typing rule for
that sort of expression look like? 

\[ \drule[width=4in]{c-case-simple} \]

Mathematically, this rule checks that the scrutinee has the type of 
some datatype $[[T]]$. Then, for each case of the pattern match, this rule
looks at the pattern and its type to calculate a telescope containing declarations of 
all of the variables bound in that pattern. That telescope is added to the context
to type check each branch. Furthermore, when checking the type of each branch, we 
can refine that type because we know that the scrutinee is equal to the pattern. 

How do we implement this rule in our language? The general strategy 
is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Infer type of the scrutinee $[[a]]$ (\texttt{inferType})
\item
  Make sure that the inferred type is some type constructor $[[T]]$
  (\texttt{ensureTCon})
\item
  For each case alternative $[[pi]][[->]][[ai]]$:
\begin{itemize}
\item
  Create the declarations $[[Di]]$ for the variables in the pattern
  (\texttt{declarePat})

\item Create a list of definitions $[[Di']]$ that follow from unifying the
  scrutinee $[[a]]$ with the pattern $[[pi]]$ (\texttt{unify})

\item
  Check the body of the case $[[ai]]$ in the extended context against the
  expected type $[[A]]$ (\texttt{checkType})
\end{itemize}

\item
  Make sure that the patterns in the cases are exhaustive
  (\texttt{exhausivityCheck})
\end{enumerate}

\subsection{Datatypes with parameters}

The first extension of the above scheme is for \emph{parameterized
datatypes}. For example, in pi-forall we can define the \texttt{Maybe}
type with the following declaration. The type parameter for this
datatype \texttt{A} can be referred to in any of the telescopes for the
data constructors.

\begin{piforall}
data Maybe (A : Type) : Type where
    Nothing 
     Just of (A)
     
\end{piforall}

Because this is a dependently-typed language, the variables in the
telescope can be referred to later in the telescope. For example, with
parameters, we can implement Sigma types as a datatype, instead of
making them primitive:

\begin{piforall}
data Sigma (A: Type) (B : A -> Type) : Type
    Prod of (x:A) (B)
\end{piforall}

The general form of datatype declaration with parameters includes a
telescope for the type constructor, as well as a telescope for each of
the data constructors.

\begin{piforall}
data T D : Type where
   Ki of Di 
\end{piforall}

That means that when we check an occurrence of a type constructor, we
need to make sure that its actual arguments match up the parameters in
the telescope. For this, we can use the argument checking judgement
above.


\[ \drule[width=3in]{i-tcon} \]

We modify the typing rule for data constructors by marking the telescope
for type constructor in the typing rule, and then substituting the
actual arguments from the expected type:

\[ \drule[width=3in]{c-dcon} \]

For example, if we are trying to check the expression
\texttt{Just\ True}, with expected type \texttt{Maybe\ Bool}, we'll
first see that \texttt{Maybe} requires the telescope
\texttt{(A\ :\ Type)}. That means we need to substitute \texttt{Bool}
for \texttt{A} in \texttt{(\_\ :\ A)}, the telescope for \texttt{Just}.
That produces the telescope \texttt{(\_\ :\ Bool)}, which we'll use to
check the argument \texttt{True}.

In \texttt{TypeCheck.hs}, the function
\begin{verbatim}
substTele :: Telescope -> [ Term ] -> Telescope -> TcMonad Telescope 
\end{verbatim}
implements this operation of substituting the actual data type arguments
for the parameters.

Note that by checking the type of data constructor applications (instead
of inferring them) we don't need to explicitly provide the parameters to
the data constructor. The type system can figure them out from the
provided type.

Also note that checking mode also enables \emph{data constructor
overloading}. In other words, we can have multiple datatypes that use
the same data constructor. Having the type available allows us to
disambiguate.

For added flexibility we can also add code to \emph{infer} the types of
data constructors when they are not actually parameterized (and when
there is no ambiguity due to overloading).

\subsection{Datatypes with indices}

The final step is to index our datatypes with constraints on the
parameters. Indexed types let us express inductively defined relations,
such as \texttt{beautiful} from Software Foundations.

\begin{verbatim}
Inductive beautiful : nat -> Prop :=
  b_0 : beautiful 0
| b_3 : beautiful 3
| b_5 : beautiful 5
| b_sum : forall n m, beautiful n -> beautiful m -> beautiful (n+m).
\end{verbatim}

Even though \texttt{beautiful} has type
\texttt{nat\ -\textgreater{}\ Prop}, we call \texttt{nat} this argument
an index instead of a parameter because it is determined by each data
constructor. It is not used uniformly in each case.

In \pif, we'll implement indices by explicitly \emph{constraining}
parameters. These constraints will just be expressed as equalities written in
square brackets. In other words, we'll define \texttt{beautiful} this way:

\begin{piforall}
data Beautiful (n : Nat) : Type where
  B0 of [n = 0]
  B3 of [n = 3]
  B5 of [n = 5]
  Bsum of (m1:Nat)(m2:Nat)(Beautiful m1)(Beautiful m2)[m = m1+m2]   
\end{piforall}

Constraints can appear anywhere in the telescope of a data constructor.
However, they are not arbitrary equality constraints---we want to
consider them as deferred substitutions. So therefore, the term on the
left must always be a variable.

These constraints interact with the type checker in a few places:

\begin{itemize}
\item
  When we use data constructors we need to be sure that the constraints
  are satisfied, by appealing to definitional equality when we are
  checking arguments against a telescope (in \texttt{tcArgTele}).

\[ \drule{tele-def} \]

\item
  When we substitute through telescopes (in \texttt{doSubst}), we may
  need to rewrite a constraint \texttt{x\ =\ b} if we substitute for
  \texttt{x}.

\item
  When we add the pattern variables to the context in each alternative
  of a case expression, we need to also add the constraints as
  definitions (see \texttt{declarePats}).
\end{itemize}

For example, if we check an occurrence of \texttt{B3}, i.e.~

\begin{verbatim}
threeIsBeautiful : Beautiful 3
threeIsBeautiful = B3
\end{verbatim}

this requires substituting \texttt{3} for \texttt{n} in the telescope
\texttt{{[}n\ =\ 3{]}}. That produces an empty telescope.

\subsubsection{Homework: Parameterized datatypes and proofs:
logic}

Translate the definitions and proofs in
\href{http://www.cis.upenn.edu/~bcpierce/sf/current/Logic.html}{Logic
chapter of Software Foundations} to \pif. See
\href{soln/test/Logic.pi}{Logic.pi} for a start.

\subsubsection{\texorpdfstring{Homework: Indexed datatypes: finite
numbers in
\texttt{FinHw.pi}
}{Homework: Indexed datatypes: finite numbers in FinHw.pi}
}


The module \texttt{FinHw.pi} declares the type of numbers that are drawn
from some bounded set. For example, the type \texttt{Fin\ 1} only
includes 1 number (called Zero), \texttt{Fin\ 2} includes 2 numbers,
etc. More generally, \texttt{Fin\ n} is the type of all natural numbers
smaller than \texttt{n}, i.e.~of all valid indices for lists of size
\texttt{n}.

\begin{piforall}
data Fin (n : Nat) : Type where
   Zero of (m:Nat)[n = Succ m] 
   Succ of (m:Nat)[n = Succ m] (Fin m)     
\end{piforall}

The file \href{soln/test/FinHw.pi}{FinHw.pi} includes a number of definitions
that use these types. Two of these definitions are marked with
\texttt{TRUSTME}. Replace these expressions with appropriate definitions.

\subsection{Erasure and datatypes}

What about putting it in data structures? We should be able to define
datatypes with ``specificational arguments''. For example, see
\href{soln/test/Vec.pi}{Vec.pi}.

Note: we can only erase \emph{data} constructor arguments, not types
that appear as arguments to \emph{type} constructors. (Parameters to
type constructors must always be relevant, they determine the actual
type.) On the other hand, datatype parameters are never relevant to data
constructors---we don't even represent them in the abstract syntax.

\subsubsection{Homework: Erasure and Indexed datatypes:
finite numbers in \texttt{FinHw.pi} }

Now take your code in \texttt{FinHw.pi} and see if you can mark some of
the components of the \texttt{Fin} datatype as erasable.


\section{Where to go for more}
\label{sec:related-work}

\subsection{Related work for \pif}

\paragraph{Section~\ref{sec:simple}:  Core Dependent Types}

\begin{itemize}
\item Barendregt, Lambda Calculi with Types

\item Cardelli, A polymorphic lambda calculus with Type:Type
\url{http://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-10.pdf}

\item Augustsson, Cayenne -- a Language With Dependent Types
\url{http://dl.acm.org/citation.cfm?id=289451}
\end{itemize}

\paragraph{Section~\ref{sec:bidirectional}:  Bidirectional type checking}

\begin{itemize}
\item Pierce and Turner

\item Peyton Jones, Vytiniotis, Weirich, Shields. Practical type inference for arbitrary-rank types
\url{https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/putting.pdf}

\item Christiansen, Tutorial on Bidirectional Typing.
\url{https://www.davidchristiansen.dk/tutorials/bidirectional.pdf}

\item Dunfield and Krishnaswami, Bidirectional Typing
\url{https://www.cl.cam.ac.uk/~nk480/bidir-survey.pdf}
\end{itemize}

\paragraph{Section~\ref{sec:equality}: Normalization by evaluation}

\begin{itemize}
\item Andreas Abel's habilitation thesis.

\item Daniel Gratzer, NBE for MLTT
\url{https://github.com/jozefg/nbe-for-mltt}
\end{itemize}

\paragraph{Section~\ref{sec:pattern-matching}: Dependently-typed pattern matching}

\begin{itemize}
\item
  Coq pattern matching:
  \href{http://coq.inria.fr/refman/Reference-Manual006.html\#Cic-inductive-definitions}{Coq
  User manual}
\item
  Agda pattern matching:
  \href{http://www.cse.chalmers.se/~ulfn/papers/thesis.pdf}{Ulf Norell's
  dissertation}
\item
  Haskell GADTs:
  \href{http://research.microsoft.com/apps/pubs/default.aspx?id=162516}{Dimitrios
  Vytiniotis, Simon Peyton Jones, Tom Schrijvers, and Martin Sulzmann,
  OutsideIn(X): Modular type inference with local assumptions}
\end{itemize}


\paragraph{Section~\ref{sec:irrelevance}: Compile-time and runtime irrelevance}

* Pfenning, Abel and Scherer
* ICC, Mishra-Linger and Sheard
* McBride, Atkey
* DDC

\subsection{Other tutorials on the implementation of dependent type systems}

\begin{itemize}
\item A. Löh, C. McBride, W. Swierstra, 
\emph{A tutorial implementation of a dependently typed lambda calculus} \cite{loeh:tutorial}.

``LambdaPi''. Implementation in Haskell.
Starts with simply-typed lambda calculus, uses locally nameless representation
and bidirectional typing, extends to core type system with type-in-type, implements NBE, then
adds natural numbers and vectors. 

\item Andrej Bauer, \emph{How to implement dependent type theory} \cite{bauer:tutorial}

Series of blog posts. Implementation in OCaml. Infinite hierarchy of type universes. Uses named representation and generates fresh names during substitution and alpha-equality. Uses full normalization to implement definitional equality. Then in second version revises to use NBE. The third version revises to use de Bruijn indices, explicit substitutions and then switches back to weak-head normalization.

\item Tiark Rompf, \emph{Implementing Dependent Types}. \cite{rompf:tutorial}.

Uses Javascript. Implements a core dependent type theory using HOAS (tagless final) and Normalization by evaluation.

\item Lennart Augustsson. \emph{Simple, Easier!} \cite{augustsson:tutorial}

Implemented in Haskell. Goal is simplest implementation. Uses string representation of variables and weak-head normalization for implementation of equality. Implementation of Barendregt's lambda cube.

\item Coquand, Kinoshita, Nordstrom, Takeyama. \emph{A simple type-theoretic
    language: Mini-TT} ~\cite{coquand:tutorial}

``Mini-TT''. Implemented in Haskell. Includes sigma types, void, unit and sums, but not indexed datatypes (or propositional equality). Uses NbE for conversion checking. Does not enforce termination.

\item Andras Kovacs, Elaboration Zoo
\url{https://github.com/AndrasKovacs/elaboration-zoo/}

\item Brigitte Pientka
\url{https://www.cs.mcgill.ca/~bpientka/papers/recon-jfp.pdf}

\end{itemize}

%\section{References}

\bibliographystyle{alpha}
\bibliography{weirich}


%\appendix

%\section{Full specification of the system}




\end{document}
